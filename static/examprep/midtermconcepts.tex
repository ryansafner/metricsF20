\documentclass{article}
\usepackage{amssymb, amsmath, booktabs, caption}
\usepackage[margin=1in]{geometry}
\usepackage{tikz, pgfplots}
\usetikzlibrary{shapes,arrows}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\title{Econometrics Midterm Concepts}
\author{Ryan Safner}
\date{ECON 480}

\begin{document}
	
\maketitle

\section*{Ordinary Least Squares (OLS) Regression}

\begin{itemize}
	\item	Bivariate data and associations between variables (e.g. $X$ and $Y$)
	\begin{itemize}
		\item Apparent relationships are best viewed by looking at a scatterplot
		\begin{itemize}
			\item Check for associations to be positive/negative, weak/strong, linear/nonlinear, etc
			\item $Y$: dependent variable
			\item $X$: independent variable 
		\end{itemize}
		\item Correlation coefficient ($r$) can quantify the strength of an association
		\begin{equation*}
					r=\frac{1}{n-1} \sum^n \bigg(\frac{X_i-\bar{X}}{s_X}\bigg) \bigg(\frac{Y_i-\bar{Y}}{s_Y}\bigg) = \frac{\displaystyle \sum^n Z_X Z_Y}{n-1}	
		\end{equation*}
		\begin{itemize}
			\item $-1 \leq r \leq 1$ and $r$ only measures \emph{linear} associations
			\item $|r|$ closer to 1 imply stronger correlation (near a perfect straight line)  
			\item Correlation does not imply causation! Might be confounding or lurking variables (e.g. $Z$) affecting $X$ and/or $Y$
		\end{itemize}
	\end{itemize}
	\item Population regression model
	\begin{equation*}
Y_i=\beta_0+\beta_1X_i+u_i
	\end{equation*}
	\begin{itemize}
		\item $\beta_1$: $\frac{\Delta Y}{\Delta X}$: the slope between $X$ and $Y$, number of units of $Y$ from a 1 unit change in $X$ 
		\item $\beta_0$ is the $Y$-intercept: literally, value of $Y$ when $X=0$
		\item $u_i$ is the error or residual, difference between actual value of $Y|X$ vs. predicted value of $\hat{Y}$
	\end{itemize}
		\begin{figure}[h!]
			\centering 
				\begin{tikzpicture}[scale=.75]\small  
				\draw[->] (0,0) -- (11,0) coordinate (x axis) node[right]{$X$};
 				\draw[->] (0,0) -- (0,11) coordinate (y axis) node[above]{$Y$};	
            	\draw[ultra thick, dashed, red] (5,5)--node[right]{$u_i = Y_i-\hat{Y}_i$}(5,6.75);
            	\draw[fill=black](1,4) circle(0.125cm);
            	\draw[fill=black](2,6) circle(0.125cm);
            	\draw[fill=black](4,2) circle(0.125cm);
            	\draw[fill=black](4,7) circle(0.125cm);
            	\draw[fill=black](5,5) circle(0.125cm)node[below]{($X_i,Y_i)$};
            	\draw[fill=black](5,5) circle(0.125cm);
            	\draw[fill=black](6,8) circle(0.125cm);
            	\draw[fill=black](7,9) circle(0.125cm);
            	\draw[fill=black](8,9) circle(0.125cm);
            	\draw[fill=black](9,10) circle(0.125cm);
            	\draw[fill=black](3,8) circle(0.125cm);
            	\draw[ultra thick, blue] (0,3)node[left]{$\beta_0$}--(9.5,10)node[right]{$\hat{Y}=\beta_0+\beta_1 X$};
            	\draw[very thick, blue, dashed] (7,8)--node[below]{$1$}(8,8)--node[right]{$\beta_1$}(8,9);
            	\draw[thick, dotted] (0,5)node[left]{$Y_i$}--(5,5)--(5,0)node[below]{$X_i$};
            	\draw[thick, dotted] (0,6.75)node[left]{$\hat{Y_i}$}--(5,6.75);
 			\end{tikzpicture}	
	\end{figure}
	\item Ordinary Least Squares (OLS) regression model 
		\begin{equation*}
\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i
		\end{equation*}
	\begin{itemize}
		\item Least square estimators $\hat{\beta_0}$ and $\hat{\beta_1}$ estimate population regression line from sample data
		\item Minimize sum of squared errors (SSE) $min\displaystyle \sum^n u_i^2$ where $u_i=Y_i-\hat{Y_i}$
		\item OLS regression line
		\begin{equation*}
 \hat{\beta_1} = \frac{cov(X,Y)}{var(X)}=\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}=r_{X,Y}\frac{s_Y}{s_X}
		\end{equation*}
		\begin{equation*}
\hat{\beta_0} = \bar{Y}-\hat{\beta_1}\bar{X}		\end{equation*}
		\end{itemize}
\clearpage 
	\item Measures of Fit
	\begin{itemize}
		\item $R^2$: fraction of total variation on $Y$ explained by variation in $X$ according to model
		\begin{equation*}
		R^2=\frac{ESS}{TSS}	
		\end{equation*}
		\begin{equation*}
		R^2=1-\frac{SSE}{TSS}	
		\end{equation*}
		\begin{equation*}
		R^2=r_{X,Y}^2	
		\end{equation*}
		\begin{itemize}
			\item $ESS = \sum (\hat{Y_i}-\bar{Y})^2$
			\item $TSS = \sum(Y_i-\bar{Y})^2$
			\item $SSE = \sum \hat{u_i}^2$ 
		\end{itemize}
		\item Standard error of the regression (SER): average size of $u_i$, average distance from regression line to data points
		\begin{equation*}
		SER=\frac{1}{n-2}\sum \hat{u_i}^2 = \frac{SSE}{n-2}
		\end{equation*}
	\end{itemize}
	\item Hypothesis testing of $\beta_1$
	\begin{itemize}
		\item $H_0: \beta_1=\beta_{1,0}$, often $H_0: \beta_1=0$
		\item Two sided alternative $H_1: \beta_1 \neq 0$
		\item One sided alternatives $H_1: \beta_1 > 0$, $H_2: \beta_1 < 0$
		\item $t$-statistic
		\begin{equation*}
t=\frac{\hat{\beta_1}-\beta_{1,0}}{SE[\hat{\beta_1}]}
		\end{equation*}
		\item Compare $t$ against critical value $t$*, or compute $p$-value as usual 
		\item Confidence intervals (95\%): $\hat{\beta_1} \pm 1.96(SE[\hat{\beta_1}])$
	\end{itemize}
			\begin{figure}[h!]
		\centering 	
			\begin{tikzpicture}\scriptsize 
\begin{axis}[
  no markers, domain=-4:4, samples=100,
  axis lines*=left, xlabel=$\hat{\beta_1}$, ylabel={},
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=10cm,
  xtick=\empty, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  %grid = major
  ]
  \addplot [very thick,cyan!50!black] {gauss(0,0.5)};
  \draw [yshift=0cm, thick, dashed](axis cs:0,0)node[below]{$\beta_1$}--(axis cs:0,0.8);
  \draw[cyan!50!black](axis cs:1,0.8)node[above]{Small variance};
    \addplot [very thick,red!50!black] {gauss(0,1.5)};
  \draw[red!50!black](axis cs:2,0.3)node[above]{Large variance};
\end{axis}
\end{tikzpicture}
\caption*{\small $\hat{\beta_1}$ is a random variable, so it has its own sampling distribution with mean $E[\hat{\beta_1}]$ and standard error $se[\hat{\beta_1}]$}
	\end{figure}
	\item Mean of OLS estimator $\hat{\beta_1}$ \& Bias: Endogeneity \& Exogeneity
	\begin{itemize}
		\item $X$ is \textbf{exogenous} if it is not correlated with the error term 
	\begin{equation*}
 	corr(X, u)=0
 	\end{equation*}
		\begin{itemize}
			\item Equivalently, knowing $X$ should not give you any information about $u$:
				\begin{equation*}
 	E[u|X]=0
 	\end{equation*}
			\item If $X$ is exogenous, OLS estimate on $X$ is unbiased: 
							\begin{equation*}
 	E[\hat{\beta_1}]=\beta_1
 	\end{equation*}
		\end{itemize}
		\item $X$ is \textbf{endogenous} if it is correlated with the error term
	\begin{equation*}
 	corr(X, u) \neq 0
 	\end{equation*}
		\begin{itemize}
			\item Equivalently, knowing $X$ gives you information about $u$: 
				\begin{equation*}
 	E[u|X] \neq 0
 	\end{equation*}
			\item If $X$ is endogenous, OLS estimate on $X$ is biased: 
			\begin{equation*}
E[\hat{\beta_1}]=\beta_1+corr(X,u)\frac{\sigma_u}{\sigma_X}
		\end{equation*}
			\begin{itemize}
				\item Can measure strength and direction ($+$ or $-$) of bias 
				\item Note: if unbiased, $corr(X,u)=0$, so $E[\hat{\beta_1}]=\beta_1$
			\end{itemize}
		\end{itemize}
	\end{itemize}  
	\item Variance of OLS estimator $\hat{\beta_1}$, measuring precision of estimate
			\begin{equation*}
		var[\hat{\beta_1}]=\frac{\hat{\sigma}^2}{n\times var(X)}
		\end{equation*}
		and standard error 
	\begin{equation*}
		se[\hat{\beta_1}]=\sqrt{\frac{\hat{\sigma}^2}{n\times var(X)}}
		\end{equation*}
	\begin{itemize}
	\item Affected by 3 major factors: 
	\begin{enumerate}
		\item Model fit, where SER=$\hat{\sigma}$
		\item Sample size $n$
		\item Variation in $X_j$
	\end{enumerate}
	\end{itemize}
	\item Heteroskedasticity and homoskedasticity
	\begin{itemize}
		\item Homoskedastic errors ($u$) have the same variance over all values of $X$
		\item Heteroskedastic errors ($u$) have different variance over values of $X$
		\begin{itemize}
			\item Heteroskedasticity does \emph{not} bias our estimates, but incorrectly lowers variance \& standard errors (inflating $t$-statistics and significance!) 
			\item Can correct for heteroskedasticity by using robust standard errors 
		\end{itemize}
	\end{itemize} 
\end{itemize}

\end{document}