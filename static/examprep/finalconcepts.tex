\documentclass{article}
\usepackage{amssymb, amsmath, booktabs, caption}
\usepackage[margin=1in]{geometry}
\usepackage{tikz, pgfplots}
\usetikzlibrary{shapes,arrows}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\title{Multivariate Regression Concepts}
\author{Ryan Safner}
\date{ECON 480 - Econometrics - Final Exam}

\begin{document}
	
\maketitle

\section*{Multivariate Regression}

\begin{itemize}
	\item Omitted Variable Bias 
	\begin{itemize}
		\item A variable $Z$ causes omitted variable bias if: 
		\begin{enumerate}
			\item $corr(X,Z) \neq 0$, $X$ and $Z$ are correlated
			\item $corr(Z, Y) \neq 0$, $Z$ is in the error term that explains $Y$	
		\end{enumerate}
		\item Omitted variable bias can be avoided by including $Z$ in the regression (as $X_2$) 
	\end{itemize}	
	\item Multivariate Regression Model
	\begin{equation*}
	\widehat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_{1i}+\hat{\beta_2}X_{2i}+\hat{\epsilon_i}	
	\end{equation*}
	\begin{itemize}
		\item $\hat{\beta_0}$: predicted value of $\hat{Y_i}$ when $X_{1i}=0; X_{2i}=0$
		\item $\hat{\beta_1}=\displaystyle\frac{\Delta Y_i}{\Delta X_{1i}}$, marginal effect of $X_{1i}$ on $Y_i$, holding $X_{2i}$ constant
		\item $\hat{\beta_2}=\displaystyle\frac{\Delta Y_i}{\Delta X_{2i}}$, marginal effect of $X_{2i}$ on $Y_i$, holding $X_{1i}$ constant
	\end{itemize} 
	\item Measuring Omitted Variable Bias
	\begin{itemize}
		\item Suppose we omit $X_{2i}$ and run an Omitted Regression
		\begin{equation*}
	Y_i=\alpha_0+\alpha_1X_{1i}+\nu_{i}	
		\end{equation*}
		\item If we run an Auxiliary Regression of $X_{2i}$ on $X_{1i}$:
		\begin{equation*}
			X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i	
		\end{equation*}
		\begin{itemize}
			\item Size and significance of $\delta_1$ measures relationship between $X_{1i}$ and $X_{2i}$
		\end{itemize}
		\begin{equation*}
		\alpha_1 = \beta_1 + \beta_2 \delta_1
		\end{equation*}
		\item Biased estimate $\alpha_1$ in Omitted Regression picks up:
		\begin{itemize}
			\item True effect of $X_{1i}$ on $Y_i$ ($\beta_1$)
			\item Effect of $X_{2i}$ on $Y_i$ ($\beta_2)$ as pulled through the relationship between $X_{1i}$ and $X_{2i}$ ($\delta_1$)
		\end{itemize}
		\item Conditions for $Z$ being an omitted variable 
		\begin{itemize}
			\item $Z_i$ must be a determinant of $Y_i$ ($\beta_2 \neq 0$)
			\item $Z_i$ is correlated with $X_{1i}$ ($\delta_1 \neq 0$)
		\end{itemize}
	\end{itemize}
			\begin{figure}[h!]
		\centering 	
			\begin{tikzpicture}\scriptsize 
\begin{axis}[
  no markers, domain=-4:4, samples=100,
  axis lines*=left, xlabel=$\hat{\beta_j}$, ylabel={},
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=10cm,
  xtick=\empty, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  %grid = major
  ]
  \addplot [very thick,cyan!50!black] {gauss(0,0.5)};
  \draw [yshift=0cm, thick, dashed](axis cs:0,0)node[below]{$\beta_j$}--(axis cs:0,0.8);
  \draw[cyan!50!black](axis cs:1,0.8)node[above]{Small variance};
    \addplot [very thick,red!50!black] {gauss(0,1.5)};
  \draw[red!50!black](axis cs:2,0.3)node[above]{Large variance};
\end{axis}
\end{tikzpicture}
\caption*{\small $\hat{\beta_j}$ is a random variable, so it has its own sampling distribution with mean $E[\hat{\beta_j}]$ and standard error $se[\hat{\beta_j}]$}
	\end{figure}
	\item Variance of OLS estimators $\hat{\beta_j}$
			\begin{equation*}
		var[\hat{\beta_j}]=\frac{1}{(1-R^2_j)}*\frac{\hat{\sigma}^2}{n\times var[X_j]}
		\end{equation*}
	and Standard error 
		\begin{equation*}
		s.e.[\hat{\beta_j}]=\sqrt{var[\hat{\beta_j}]}
		\end{equation*}
	\item Affected by 4 major factors: 
	\begin{enumerate}
		\item Model fit, where SER=$\hat{\sigma}$
		\item Sample size $n$
		\item Variation in $X_j$
		\item Variance Inflation Factor (VIF) $\frac{1}{1-R^2_j}$
		\begin{itemize}
			\item Independent variables are \textbf{multicollinear} if they are correlated 
			\begin{equation*}
				corr(X_j, X_l) \neq 0 \text{ for } j \neq l
			\end{equation*}
			\item Does not bias estimators, but increases their variance \& standard errors 
			\item $R^2_j$ is the $R^2$ from an auxiliary regression of $X_j$ on all other regressors
			\item VIF quantifies how by many times the variance of $\hat{\beta_j}$ increased because of multicollinearity
			\begin{itemize}
				\item $VIF>10$ (or $\frac{1}{VIF}>0.10$) is bad 
			\end{itemize}
			\item \textbf{Perfect multicollinearity} when a regressor is an exact linear function of (an)other regressor(s) -- cannot run a regression, a logical impossibility 
			\begin{equation*}
			|corr(X_1, X_2)| = 1	
			\end{equation*}
		\end{itemize}
	\end{enumerate}
\end{itemize}

\clearpage 

\section*{Dummy Variables}

\begin{itemize}
	\item Dummy variable 		\[D_i = \left\{
 		 \begin{array}{ll}
    		1 & \text{if } i \text{ meets  condition} \\
   			0 & \text{if } i \text{ does not meet condition }\\
  		\end{array}
  		\right.
		\]
	\item Dummy variables measure group means 
		\begin{equation*}
	\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1} D_i
	\end{equation*}
	\begin{itemize}
		\item When $D_i=0$ (Control group):
		\begin{itemize}
			\item $\hat{Y_i}=\hat{\beta_0}$
			\item $E[Y|D_i=0]=\hat{\beta_0}$ $\iff$ the mean of $Y$ when $D_i=0$
		\end{itemize}
		\item When $D_i=1$ (Treatment group):
		\begin{itemize}
			\item $\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1} D_i$
			\item $E[Y|D_i=1]=\hat{\beta_0}+\hat{\beta_1}$ $\iff$ the mean of $Y$ when $D_i=1$
		\end{itemize}
		\item Difference in group means:
		\begin{align*}
		&=E[Y_i|D_i=1]-E[Y_i|D_i=0]\\
		&=(\hat{\beta_0}+\hat{\beta_1})-(\hat{\beta_0})\\
		&=\hat{\beta_1}\\
		\end{align*}
	\end{itemize}
	\item Transforming categorical variables into dummies 
	\begin{itemize}
		\item A categorical variable (e.g. region, class standing, etc) can be added to a regression by making each category option a dummy variable and including them all (minus one) 
		\begin{equation*}
		Y_i=\beta_0+\beta_2 D_1 + \beta_2 D_2 + \beta_3 D_3	
		\end{equation*}
		where observations can fall into category 1, 2, 3, or 4
		\item Including all category option dummies into a regression yields the \textbf{dummy variable trap}, where all dummies are perfectly multicollinear
		\item Must drop one category dummy, the ``reference group''
		\item Coefficients on dummy variables are the difference between that category and the reference category: 
		\begin{itemize}
			\item $\beta_0=Y$ for category 4 (omitted)
			\item $\beta_1=$ difference between category 1 and category 4 (omitted)
			\item $\beta_2=$ difference between category 2 and category 4 (omitted)
			\item $\beta_3=$ difference between category 3 and category 4 (omitted)
	\end{itemize}
	\end{itemize}
	\item \textbf{Interaction terms} measure if there is an additional effect of one variable on the value of another, 3 combinations:
	\begin{enumerate}
		\item Between a dummy and a continuous variable 
		\begin{equation*}
		Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 \mathbf{X_i \times D_i}
		\end{equation*}
		\begin{itemize}
		\item Coefficients: 
		\begin{itemize}
			\item $\beta_0$: $Y_i$ for $X_i=0$ and $D_i=0$
			\item $\beta_1$: Effect of $X_i \rightarrow Y_i$ for $D_i=0$
			\item $\beta_2$: Effect on $Y_i$ of difference between $D_i=0$ and $D_i=1$
			\item $\beta_3$: Effect of \emph{difference} of $X_i \rightarrow Y_i$ between $D_i=0$ and $D_i=1$
		\end{itemize} 
		\item Easier to see as two different regression lines: 
		\begin{itemize}
		\item When $D_i=0$ (Control group):
			\begin{equation*}
			\textcolor{magenta}{\hat{Y_i} =\hat{\beta_0}+\hat{\beta_1}X_i}
			\end{equation*}
		\item When $D_i=1$ (Treatment group):
			\begin{equation*}
			\textcolor{teal}{\hat{Y_i} =(\hat{\beta_0}+\hat{\beta_2})+(\hat{\beta_1}+\hat{\beta_3})X_i}
			\end{equation*}
			\begin{figure}[h!]
				\centering 
				\begin{tikzpicture}[scale=.5]\scriptsize 
				\draw[->] (0,0) -- (11,0) coordinate (x axis) node[right]{$X$};
 				\draw[->] (0,0) -- (0,11) coordinate (y axis) node[above]{$Y$};	
            	\draw[ultra thick, magenta] (0,2)node[left]{$\hat{\beta_0}$}--node[below]{slope=$\hat{\beta_1}$}(10,5);
            	\draw[ultra thick, teal] (0,4)node[left]{$\hat{\beta_0}+\hat{\beta_2}$}--node[above=.5cm]{slope=$\hat{\beta_1}+\hat{\beta_3}$}(10,10);
 			\end{tikzpicture}	
	\end{figure}
 			\item Two regression lines may have (same/different) intercepts and (same/different) intercepts, test significance of: 
 			\begin{itemize}
 				\item $\beta_2$: difference in intercepts
 				\item $\beta_3$: difference in slopes 
 			\end{itemize}
		\end{itemize} 
		\end{itemize} 
		\item Between two dummy variables 
	\begin{equation*}
	Y_i=\beta_0+\beta_1D_{1i}+\beta_2D_{2i}+\beta_3\mathbf{D_{1i}\times D_{2i}}
	\end{equation*}
		\begin{itemize}
			\item Coefficients: 
			\begin{itemize}
			\item $\beta_0$: value of $Y$ for $D_{1i}=0$ and $D_{2i}=0$
			\item $\beta_1$: effect on $Y$ of $D_{1i}=0 \rightarrow 1$ when $D_{2i}=0$
			\item $\beta_2$: effect on $Y$ of $D_{2i}=0 \rightarrow 1$ when $D_{1i}=0$
			\item $\beta_3$: \emph{increment} to effect on $Y$ of $D_{1i}=0 \rightarrow 1$ when $D_{2i}=1$ vs. when $D_{2i}=0$
			\end{itemize}
			\item Compare difference in group means:
		\begin{itemize}
			\item $D_{1i}=0, D_{2i}=0$: $\widehat{Y_i}=\hat{\beta_0}$
			\item $D_{1i}=0, D_{2i}=1$: $\widehat{Y_i}=\hat{\beta_0}+\hat{\beta_2}$
			\item $D_{1i}=1, D_{2i}=0$: $\widehat{Y_i}=\hat{\beta_0}+\hat{\beta_1}$
			\item $D_{1i}=1, D_{2i}=1$: $\widehat{Y_i}=\hat{\beta_0}+\hat{\beta_1}+\hat{\beta_2}+\hat{\beta_3}$
		\end{itemize}
		\end{itemize}
		\item Between two continuous variables
		\begin{equation*}
		Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+\beta_3\mathbf{(X_{1i} \times X_{2i})}	
		\end{equation*}
		\begin{itemize}
			\item Marginal effects: 
			\begin{itemize}
			\item $\displaystyle\frac{\Delta Y_i}{\Delta X_{1i}}=\beta_1+\beta_3 X_{2i}$ --- marginal effect of $X_{1i} \rightarrow Y_i$ depends on $X_{2i}$
			\item $\displaystyle\frac{\Delta Y_i}{\Delta X_{2i}}=\beta_2+\beta_3 X_{1i}$ --- marginal effect of $X_{2i} \rightarrow Y_i$ depends on $X_{1i}$
			\end{itemize} 
		\end{itemize}
	\end{enumerate}
\end{itemize}

\clearpage 

\section*{Transforming Variables}

\begin{itemize}
	\item Polynomial functions
	\begin{equation*}
	\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\beta_2} X_i^2 + ... + \hat{\beta_{r}} X_i^{r} + \epsilon_i
	\end{equation*}
	where $r$ is highest power $X_i$ is raised to, a function with $r-1$ bends
	\begin{itemize}
		\item Quadratic model  
	\begin{equation*}
	\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\beta_2} X_i^2 + \epsilon_i
	\end{equation*}
	\begin{itemize}
		\item Marginal effect of $X_i \rightarrow Y_i$:
		\begin{equation*}
			\frac{d \; Y_i}{d \; X_i}=\hat{\beta_1}+2\hat{\beta_2}X_i
		\end{equation*}
		\item Value of $X_i$ where $Y_i$ is minimized/maximized: 
		\begin{equation*}
			X_i^*=-\frac{1}{2}\frac{\beta_1}{\beta_2}
		\end{equation*}
	\end{itemize}
	\item To determine if a higher-powered term is necessary, test significance of its associated coefficient (e.g. $\beta_2$ for quadratic model above)
	\item To determine if a model is nonlinear, run $F$-test of all higher-powered terms 
	\end{itemize}	
	\item Logarithmic functions (ln)
	\begin{itemize}
		\item Natural Logs (ln) are used to talk about percentage changes, 3 types of models: 
		\begin{enumerate}
			\item Linear-log model: 
			\begin{equation*}
			Y=\beta_0+\beta_1 \mathbf{ln(X)}	
			\end{equation*}
			\begin{itemize}
				\item $\beta_1$: A 1\% change in $X$ $\rightarrow$ $\frac{\beta_1}{100}$ unit change in $Y$
			\end{itemize}
			\item Log-linear model:
			\begin{equation*}
			\mathbf{ln(Y)}=\beta_0+\beta_1X	
			\end{equation*}
			\begin{itemize}
				\item $\beta_1$: A 1 unit change in $X$ $\rightarrow$ $100 \times \beta_1$\% change in $Y$
			\end{itemize}
			\item Log-log model:
			\begin{equation*}
			\mathbf{ln(Y)}=\beta_0+\beta_1\mathbf{ln(X)}	
			\end{equation*}		
			\begin{itemize}
				\item $\beta_1$: A 1\% change in $X$ $\rightarrow\beta_1$\% change in $Y$ (\textbf{elasticity} between $X$ and $Y$)
			\end{itemize}
		\end{enumerate}
	\end{itemize}
	\item Standardized coefficients
	\begin{equation*}
	Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}
	\end{equation*}
	\begin{itemize}
		\item To compare the magnitude of marginal effects (e.g. is $\beta_1 > \beta_2$) across variables of different units, standardize the variables by taking the $Z$-score of all observations
		\begin{equation*}
		Variable_{std}=\frac{Variable-\overline{Variable}}{sd(Variable)}	
		\end{equation*}
		\item Coefficients measure the \# of standard deviations change of $Y$ a 1 std. dev. change in $X$ causes
	\end{itemize}
	\item Joint Hypothesis Testing
	\begin{itemize}
		\item Joint hypothesis tests against the null hypothesis of a value for multiple parameters, e.g. 
		\begin{align*}
		H_0\text{:} & \beta_1=0, \beta_2=0\\
		H_1\text{:} & H_0 \text{ is false} \\
		\end{align*}
		\item Three common tests
		\begin{enumerate}
			\item $H_0$: $\beta_1=\beta_2=0$	, testing if multiple variables do not affect $Y$
			\item $H_0$: $\beta_1=\beta_2$, testing if multiple variables have the same effect (must be same units)
			\item $H_0$: all $\beta$'s$=0$, the model overall explains no variation in $Y$
		\end{enumerate}
		\item In general, with $q$ restrictions:
		\begin{equation*}
		H_0: \beta_j=\beta_{j,0}, \beta_k=\beta_{k,0}, ...\text{for }q \text{ restrictions}	
		\end{equation*}
		\item Use the $F$-statistic, (simplified homoskedastic formula below)
		\begin{equation*}
	F_{q,n-k-1}=\cfrac{\bigg(\displaystyle\frac{(R^2_u-R^2_r)}{q}\bigg)}{\bigg(\displaystyle\frac{(1-R^2_u)}{(n-k-1)}\bigg)}
	\end{equation*}
		\item Compares the $R^2$'s of two models:
		\begin{itemize}
			\item Unrestricted model: regression with all coefficients
			\item Restricted model: regression under the null hypothesis (e.g. where $\beta_1=0, \beta_2=0$)
	\end{itemize}
	\item $F$ tests if the increase in $R^2$ from including the suspect variables ($Restricted \rightarrow Unrestricted$) increases by a statistically significant amount
\end{itemize}
\end{itemize}
\clearpage 

\section*{Panel Data}
\begin{itemize}
	\item Panel data tracks the same individuals (a cross-section) over time (time-series)
		\begin{equation*}
	\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\epsilon_{it}
	\end{equation*}
	with $N$ number of $i$ groups and $T$ number of $t$ time periods	
	\item A \textbf{pooled model} simply runs this as normal OLS regression
	\begin{itemize}
		\item Biased: ignores factors correlated with $X$ in $\epsilon$
		\item Systematic differences across groups $i$ that may be stable over time
		\item Systematic differences across time $t$ that may be stable across groups
	\end{itemize}
	\item (One-Way) Fixed effects model 
	\begin{equation*}
	\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\mathbf{\alpha_i}+\nu_{it}\\
	\end{equation*}
	\begin{itemize}
		\item  $\alpha_i$: group-fixed effect (pulled from error term $\epsilon_{it}$)  
		\begin{itemize}
			\item Includes \emph{all} differences across groups that do not change over time! (e.g. geography, culture, etc. of Maryland vs. Alaska)
			\item Does \emph{not} include variables that change over time! 
			\item Estimates a different intercept for each group
		\end{itemize}
		\item Least Squares Dummy Variable (LSDV) Approach: can estimate via creating \& including a dummy variable for each group (minus 1 to avoid dummy variable trap)
		\begin{equation*}
			\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\sum_{i=1}^{N-1} \alpha_i D_{i}
		\end{equation*}
			where $\alpha_i$ is a coefficient and $D_i$ is a dummy variable for group $i$, for example: 
			\begin{equation*}
				\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\beta_2 Alabama_i+\beta_3 Alaska_i+...
			\end{equation*}

	\end{itemize}
	\item Two-Way Fixed effects model 
	\begin{equation*}
	\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\mathbf{\alpha_i}+\mathbf{\tau_t}+\nu_{it}
	\end{equation*}
	\begin{itemize}
		\item  $\tau_i$: time-fixed effect (pulled from error term $\epsilon_{it}$)  
		\begin{itemize}
			\item Includes \emph{all} differences over time that do not change across groups! (e.g. all States experience recession in 2008, or federal law change)
			\item Does \emph{not} include variables that are different across groups! 
			\item Estimates a different intercept for each time period
		\end{itemize}
		\item Least Squares Dummy Variable (LSDV) Approach: can estimate via creating \& including a dummy variable for each group and each time period (minus 1 for each to avoid dummy variable trap)
		\begin{equation*}
			\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\sum_{i=1}^{N-1} \alpha_i D_{i}+\sum_{t=1}^{T-1} \tau_i D_{t}
		\end{equation*}
		where $\alpha_i$ and $\tau_t$ are coefficients, $D_i$ is a dummy variable for group $i$, and $D_t$ is a dummy variable for time period $t$, for example: 
			\begin{equation*}
				\widehat{Y_{it}}=\beta_0+\beta_1X_{it}+\beta_2 Alabama_i+\beta_3 Alaska_i+... +\beta_{51}2000_t + \beta_{52}2001_t + ...
			\end{equation*}
	\end{itemize}
\clearpage 
	\item Difference-in-Differences model
	\begin{equation*}
	\widehat{Y_{it}}=\beta_0+\beta_1 \text{Treated}_i +\beta_2 \text{After}_{it}+\beta_3 (\text{Treated}_i \times \text{After}_{t})+\epsilon_{it}
	\end{equation*}
		\begin{itemize}
		\item Where: 
			\begin{itemize}
				\item Treated$_i=1$ if unit $i$ is in treatment group
				\item After$_{it}=1$ if observation $it$ is after treatment period 
			\end{itemize}
\begin{table}[h!]\small 
		\centering 
		\begin{tabular}{rrrr}
		 & Control & Treatment & Group Diff. ($\Delta Y_i$)\\ \toprule 
		 Before & $\beta_0$ & $\beta_0+\beta_1$ & $\beta_1$\\
		 After & $\beta_0+\beta_2$ & $\beta_0+\beta_1+\beta_2+\beta_3$ & $\beta_1+\beta_3$\\ \midrule 
		 Time Diff. ($\Delta Y_t$) & $\beta_2$	 & $\beta_2+\beta_3$ & $\beta_3$\\ \bottomrule 
		 & & & Diff-in-diff ($\Delta \Delta Y$)\\ 
		\end{tabular}
	\end{table}
		\begin{figure}[h!]
			\centering 
				\begin{tikzpicture}[scale=.45]\scriptsize 
				\draw[->] (-1,0) -- (11,0) coordinate (x axis) node[right]{$time$};
 				\draw[->] (-1,0) -- (-1,11) coordinate (y axis) node[above]{$Y$};	
 				\draw[fill=black](2,2)circle(0.1cm)node[below]{$C_1$};
 				\draw[fill=black](8,4)circle(0.1cm)node[above]{$C_2$};
 				\draw[fill=black](2,5)circle(0.1cm)node[above]{$T_1$};
 				\draw[fill=black](8,10)circle(0.1cm)node[above]{$T_2$};
            	\draw[ultra thick, magenta] (2,2)--(8,4);
            	\draw[ultra thick, dotted, magenta] (2,2)--(8,2)--node[right]{$\hat{\beta_2}$}(8,4);
            	\draw[ultra thick, teal] (2,5)node[left]{$\hat{\beta_0}+\hat{\beta_1}$}--(8,10);
            	\draw[ultra thick, dotted, teal] (2,5)--(8,5);
            	\draw[ultra thick, magenta](8,5)--node[right]{$\hat{\beta_2}$}(8,7);
            	\draw[ultra thick, dotted, magenta](2,5)--(8,7);
            	\draw[ultra thick, blue](8,7)--node[right]{$\hat{\beta_3}$}(8,10);
            	\draw (2,0.25)--(2,-0.25)node[below]{$t_{Before}$};
            	\draw (8,0.25)--(8,-0.25) node[below]{$t_{After}$};
            	\draw[thick, dotted](2,2)--node[left]{$\hat{\beta_1}$}(2,5);
            	\draw[thick, dotted](2,0)--(2,2)node[left]{\textcolor{magenta}{$\hat{\beta_0}$}};
 			\end{tikzpicture}	
	\end{figure}
	\begin{equation*}
	\Delta \Delta Y= (Treated_{after}-Treated_{before})-(Control_{after}-Control_{before})
	\end{equation*}
	\item OLS Coefficients: 
		\begin{itemize}
			\item $\hat{\beta_0}$: value of $Y$ for control before treatment 
			\item $\hat{\beta_1}$: difference between treatment and control (before treatment)
			\item $\hat{\beta_2}$: time difference between before and after treatment
			\item $\hat{\beta_3}$: difference-in-difference: effect of treatment    
		\end{itemize}
	\item Values of $Y$ for different groups:
		\begin{itemize}
			\item $Y$ for Control Group Before: $\hat{\beta_0}$
			\item $Y$ for Control Group After: $\hat{\beta_0}+\hat{\beta_2}$
			\item $Y$ for Treatment Group Before: $\hat{\beta_0}+\hat{\beta_1}$
			\item $Y$ for Treatment Group After: $\hat{\beta_0}+\hat{\beta_1}+\hat{\beta_2}+\hat{\beta_3}$
			\item Treatment Effect: \textcolor{blue}{$\hat{\beta_3}$}
		\end{itemize}
	\item Key assumption about \emph{counterfactual}: if not for treatment, the treated group would change the same over time as the control group (parallel time trends, magenta dotted line)
	\item Can generalize the model with  two way fixed effects: 
	\begin{equation*}
\widehat{Y_{it}}=\alpha_i +\tau_t+\beta_3 (\text{Treated}_i \times \text{After}_{t})+ X_{it}+\epsilon_{it}	
\end{equation*}
	\begin{itemize}
		\item $\alpha_i$: group-fixed effects, where some groups receive treatment and others do not
		\item $\tau_t$: time-fixed effects, where some periods are before treatment and others are after
		\item $X_{it}$: other control variables 
		\item This allows for multiple treatments to happen at different times! 
	\end{itemize}
\end{itemize}
\end{itemize}


\end{document}
