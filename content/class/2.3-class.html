---
title: "2.3 — OLS Linear Regression — Class Notes"
citeproc: false
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-syllabus-no-bib.csl
output:
  blogdown::html_page:
    template: ../../pandoc/toc-title_html.template
    toc: true
editor_options: 
  chunk_output_type: console
---

<!-- BLOGDOWN-HEAD -->
<script src="/rmarkdown-libs/accessible-code-block/empty-anchor.js"></script>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>
<!-- /BLOGDOWN-HEAD -->

<h2>Contents</h2>
<div id="TOC">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#live-class-session-on-zoom">Live Class Session on Zoom</a></li>
<li><a href="#problem-set">Problem Set</a></li>
<li><a href="#math-appendix-covariance-and-correlation">Math Appendix: Covariance and Correlation</a><ul>
<li><a href="#variance">Variance</a></li>
<li><a href="#covariance">Covariance</a></li>
<li><a href="#correlation">Correlation</a></li>
</ul></li>
<li><a href="#calculating-correlation-example">Calculating Correlation Example</a></li>
</ul>
</div>

<p><em>Thursday, September 10, 2019</em></p>
<h2 id="overview">Overview</h2>
<p>Today we start looking at <em>associations</em> between variables, which we will first attempt to quantify with measures like <em>covariance</em> and <em>correlation</em>. Then we turn to fitting a line to data via <em>linear regression</em>. We overview the basic regression model, the parameters and how they are derived, and see how to work with regressions in <code>R</code> with <code>lm</code> and the tidyverse package <a href="https://broom.tidyverse.org/"><code>broom</code></a>.</p>
<p>We consider an extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, <a href="https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-Update-3rd-Edition/9780133486872.html?tab=resources">Stock and Watson, 2007</a>. Download and follow along with the data from today’s example:<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">Note this is a <code>.dta</code> Stata file. You will need to (install and) load the package <code>haven</code> to <code>read_dta()</code> Stata files into a dataframe.<br />
<br />
</span></span></p>
<ul>
<li><a href="http://metricsf20.classes.ryansafner.com/data/caschool.dta"><i class="fas fa-table"></i> <code>caschool.dta</code></a></li>
</ul>
<h2 id="slides">Slides</h2>
<ul>
<li><a href="/slides/2.3-slides.html"><i class="fas fa-chalkboard-teacher"></i> View Lecture Slides</a></li>
<li><a href="/slides/2.3-slides.pdf"><i class="fas fa-file-pdf"></i> Download as PDF</a></li>
</ul>
<h2 id="live-class-session-on-zoom">Live Class Session on Zoom</h2>
<p>The live class <i class="fas fa-video"></i> Zoom meeting link can be found on Blackboard (see <code>LIVE ZOOM MEETINGS</code> on the left navigation menu), starting at 11:30 AM.</p>
<p>If you are unable to join today’s live session, or if you want to review, you can find the recording stored on Blackboard via Panopto (see <code>Class Recordings</code> on the left navigation menu).</p>
<h2 id="problem-set">Problem Set</h2>
<p><a href="/assignment/01-problem-set">Problem Set 1</a> answers are posted on that page in various formats.</p>
<p><a href="/assignment/02-problem-set">Problem set 2</a> (on classes 2.1-2.2) is due by 11:59 PM Sunday September 13 by upload on Blackboard.</p>
<h2 id="math-appendix-covariance-and-correlation">Math Appendix: Covariance and Correlation</h2>
<h3 id="variance">Variance</h3>
<p>Recall the variance of a <em>discrete</em> random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(var(X)\)</span> or <span class="math inline">\(\sigma^2\)</span>, is the expected value (probability-weighted average) of the squared deviations of <span class="math inline">\(X_i\)</span> from it’s mean (or expected value) <span class="math inline">\(\bar{X}\)</span> or <span class="math inline">\(E(X)\)</span>.<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">Note there will be a different in notation depending on whether we refer to a population (e.g. <span class="math inline">\(\mu_{X}\)</span>) or to a sample (e.g. <span class="math inline">\(\bar{X}\)</span>). As the overwhelming majority of cases we will deal with samples, I will use sample notation for means).<br />
<br />
</span></span></p>
<p><span class="math display">\[\begin{align*}
\sigma_X^2&amp;=E(X-E(X))\\
&amp;=\sum^n_{i=1} (X_i-\bar{X})^2 p_i
\end{align*}\]</span></p>
<p>Fpr <em>continuous</em> data (if all possible values of <span class="math inline">\(X_i\)</span> are equally likely or we don’t know the probabilities), we can write variance as a simple average of squared deviations from the mean:</p>
<p><span class="math display">\[\begin{align*}
\sigma_X^2&amp;=\frac{1}{n}\sum^n_{i=1}(X_i-\bar{X})^2  
\end{align*}\]</span></p>
<p>Variance has some useful properties:</p>
<p><strong>Property 1</strong>: The variance of a constant is 0</p>
<p><span class="math display">\[var(c)=0 \text{ iff } P(X=c)=1\]</span></p>
<p>If a random variable takes the same value (e.g. 2) with probability 1.00, <span class="math inline">\(E(2)=2\)</span>, so the average squared deviation from the mean is 0, because there are never any values other than 2.</p>
<p><strong>Property 2</strong>: The variance is unchanged for a random variable plus/minus a constant</p>
<p><span class="math display">\[var(X\pm c)\]</span></p>
<p>Since the variance of a constant is 0.</p>
<p><strong>Property 3</strong>: The variance of a scaled random variable is scaled by the square of the coefficient</p>
<p><span class="math display">\[var(aX)=a^2var(X)\]</span></p>
<p><strong>Property 4</strong>: The variance of a linear transformation of a random variable is scaled by the square of the coefficient</p>
<p><span class="math display">\[var(aX+b)=a^2var(X)\]</span></p>
<h3 id="covariance">Covariance</h3>
<p>For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we can measure their <strong>covariance</strong> (denoted <span class="math inline">\(cov(X,Y)\)</span> or <span class="math inline">\(\sigma_{X,Y}\)</span>)<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Again, to be technically correct, <span class="math inline">\(\sigma_{X,Y}\)</span> refers to populations, <span class="math inline">\(s_{X,Y}\)</span> refers to samples, in line with population vs. sample variance and standard deviation. Recall also that sample estimates of variance and standard deviation divide by <span class="math inline">\(n-1\)</span>, rather than <span class="math inline">\(n\)</span>. In large sample sizes, this difference is negligible.<br />
<br />
</span></span> to quantify how they vary <em>together</em>. A good way to think about this is: when <span class="math inline">\(X\)</span> is above its mean, would we expect <span class="math inline">\(Y\)</span> to also be above its mean (and covary positively), or below its mean (and covary negatively). Remember, this is describing the <em>joint</em> probability distribution for two random variables.</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=E\big[(X-\bar{X})(Y-\bar{Y})\big]
\end{align*}\]</span></p>
<p>Again, in the case of equally probable values for both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, covariance is sometimes written:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=\frac{1}{N}\sum_{i=1}^n(X-\bar{X})(Y-\bar{Y})
\end{align*}\]</span></p>
<p>Covariance also has a number of useful properties:</p>
<p><strong>Property 1</strong>: The covariance of a random variable <span class="math inline">\(X\)</span> and a constant <span class="math inline">\(c\)</span> is 0</p>
<p><span class="math display">\[cov(X,c)=0\]</span></p>
<p><strong>Property 2</strong>: The covariance of a random variable and itself is the variable’s variance</p>
<p><span class="math display">\[\begin{align*}
    cov(X,X)&amp;=var(X)\\
    \sigma_{X,X}&amp;=\sigma^2_X\\
    \end{align*}\]</span></p>
<p><strong>Property 3</strong>: The covariance of a two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> each scaled by a constant <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the product of the covariance and the constants</p>
<p><span class="math display">\[cov(aX,bY)=a\times b \times cov(X,Y)\]</span></p>
<p><strong>Property 4</strong>: If two random variables are independent, their covariance is 0</p>
<p><span class="math display">\[cov(X,Y)=0 \text{ iff } X \text{ and } Y \text{ are independent:}  E(XY)=E(X)\times E(Y)\]</span></p>
<h3 id="correlation">Correlation</h3>
<p>Covariance, like variance, is often cumbersome, and the numerical value of the covariance of two random variables does not really mean much. It is often convenient to normalize the covariance to a decimal between <span class="math inline">\(-1\)</span> and 1. We do this by dividing by the product of the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is known as the <strong>correlation coefficient</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, denoted <span class="math inline">\(corr(X,Y)\)</span> or <span class="math inline">\(\rho_{X,Y}\)</span> (for populations) or <span class="math inline">\(r_{X,Y}\)</span> (for samples):</p>
<p><span class="math display">\[\begin{align*}    
r_{X,Y}&amp;=\frac{cov(X,Y)}{sd(X)sd(Y)}\\
&amp;=\frac{E\big[(X-\bar{X})(Y-\bar{Y})\big]}{\sqrt{E\big[X-\bar{X}\big]}\sqrt{E\big[Y-\bar{Y}\big]}}\\
&amp;=\frac{\sigma_{X,Y}}{\sigma_X \sigma_Y}\\
\end{align*}\]</span></p>
<p>Note this also means that covariance is the product of the standard deviation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and their correlation coefficient:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=r_{X,Y}\sigma_X \sigma_Y  \\
cov(X,Y)&amp;=corr(X,Y)\times sd(X) \times sd(Y)    \\
\end{align*}\]</span></p>
<p>Another way to reach the (sample) correlation coefficient is by finding the average joint <span class="math inline">\(Z\)</span>-score of each pair of <span class="math inline">\((X_i,Y_i)\)</span>:</p>
<p><span class="math display">\[\begin{align*}    
r_{X,Y}&amp;=\frac{1}{n}\frac{\displaystyle\sum^n_{i=1}(X_i-\bar{X})(Y_i-\bar{Y}))}{s_X s_Y} &amp;&amp; \text{Definition of sample correlation}\\
&amp;=\frac{1}{n}\displaystyle\sum^n_{i=1}\bigg(\frac{X_i-\bar{X}}{s_X}\bigg)\bigg(\frac{Y_i-\bar{Y}}{s_Y}\bigg) &amp;&amp; \text{Breaking into separate sums} \\
&amp;=\frac{1}{n}\displaystyle\sum^n_{i=1}(Z_X)(Z_Y) &amp;&amp; \text{Recognize each sum is the z-score for that r.v.} \\
\end{align*}\]</span></p>
<p>Correlation has some useful properties that should be familiar to you:</p>
<ul>
<li>Correlation is between <span class="math inline">\(-1\)</span> and 1</li>
<li>A correlation of -1 is a downward sloping straight line</li>
<li>A correlation of 1 is an upward sloping straight line</li>
<li>A correlation of 0 implies no relationship</li>
</ul>
<h2 id="calculating-correlation-example">Calculating Correlation Example</h2>
<p>We can calculate the correlation of a simple data set (of 4 observations) using <code>R</code> to show how correlation is calculated. We will use the <span class="math inline">\(Z\)</span>-score method. Begin with a simple set of data in <span class="math inline">\((X_i, Y_i)\)</span> points:</p>
<p><span class="math display">\[ (1,1), (2,2), (3,4), (4,9) \]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a>corr_example&lt;-<span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>),</span>
<span id="cb1-4"><a href="#cb1-4"></a>                         <span class="dt">y=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span>))</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="kw">ggplot</span>(corr_example,<span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>y))<span class="op">+</span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="/class/2.3-class_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>corr_example <span class="op">%&gt;%</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_x =</span> <span class="kw">mean</span>(x), <span class="co">#find mean of x, its 2.5</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>            <span class="dt">sd_x =</span> <span class="kw">sd</span>(x), <span class="co">#find sd of x, its 1.291</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>            <span class="dt">mean_y =</span> <span class="kw">mean</span>(y), <span class="co">#find mean of y, its 4</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>            <span class="dt">sd_y =</span> <span class="kw">sd</span>(y)) <span class="co">#find sd of y, its 3.559</span></span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   mean_x  sd_x mean_y  sd_y
##    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1    2.5  1.29      4  3.56</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#take z score of x,y for each pair and multiply them</span></span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>corr_example &lt;-<span class="st"> </span>corr_example <span class="op">%&gt;%</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">z_product =</span> ((x<span class="op">-</span><span class="kw">mean</span>(x))<span class="op">/</span><span class="kw">sd</span>(x)) <span class="op">*</span><span class="st"> </span>((y<span class="op">-</span><span class="kw">mean</span>(y))<span class="op">/</span><span class="kw">sd</span>(y)))</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a>corr_example <span class="op">%&gt;%</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg_z_product =</span> <span class="kw">sum</span>(z_product)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">-</span><span class="dv">1</span>), <span class="co"># average z products over n-1</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>            <span class="dt">actual_corr =</span> <span class="kw">cor</span>(x,y), <span class="co">#compare our answer to actual cor() command!</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>            <span class="dt">covariance =</span> <span class="kw">cov</span>(x,y)) <span class="co"># just for kicks, what&#39;s the covariance? </span></span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   avg_z_product actual_corr covariance
##           &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
## 1         0.943       0.943       4.33</code></pre>
