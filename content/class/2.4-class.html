---
title: "2.4 — OLS: Goodness of Fit and Bias — Class Notes"
citeproc: false
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-syllabus-no-bib.csl
output:
  blogdown::html_page:
    template: ../../pandoc/toc-title_html.template
    toc: true
editor_options: 
  chunk_output_type: console
---

<!-- BLOGDOWN-HEAD -->
<script src="/rmarkdown-libs/accessible-code-block/empty-anchor.js"></script>
<!-- /BLOGDOWN-HEAD -->

<h2>Contents</h2>
<div id="TOC">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#live-class-session-on-zoom">Live Class Session on Zoom</a></li>
<li><a href="#problem-set">Problem Set</a></li>
<li><a href="#appendix-ols-estimators">Appendix: OLS Estimators</a><ul>
<li><a href="#deriving-the-ols-estimators">Deriving the OLS Estimators</a></li>
<li><a href="#algebraic-properties-of-ols-estimators">Algebraic Properties of OLS Estimators</a></li>
<li><a href="#bias-in-hatbeta_1">Bias in <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li><a href="#proof-of-the-unbiasedness-of-hatbeta_1">Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
</ul></li>
</ul>
</div>

<p><em>Tuesday, September 15, 2019</em></p>
<h2 id="overview">Overview</h2>
<p>Today we continue looking at basic OLS regression. We will cover how to measure if a regression line is a good fit (using <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\sigma_u\)</span> or SER), and whether OLS estimators are <em>biased</em>. These will depend on four critical <em>assumptions about <span class="math inline">\(u\)</span>.</em></p>
<p>In doing so, we begin an ongoing exploration into inferential statistics, which will finally become clear in another week. The most confusing part is recognizing that there is a <em>sampling distribution of each OLS estimator</em>. We want to measure the center of that sampling distribution, to see if the estimator is <em>biased</em>. Next class we will measure the spread of that distribution.</p>
<p>We continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, <a href="https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-Update-3rd-Edition/9780133486872.html?tab=resources">Stock and Watson, 2007</a>. Download and follow along with the data from today’s example:<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">Note this is a <code>.dta</code> Stata file. You will need to (install and) load the package <code>haven</code> to <code>read_dta()</code> Stata files into a dataframe.<br />
<br />
</span></span></p>
<ul>
<li><a href="http://metricsf20.classes.ryansafner.com/data/caschool.dta"><i class="fas fa-table"></i> <code>caschool.dta</code></a></li>
</ul>
<p>I have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):</p>
<ul>
<li><a href="https://rstudio.cloud/spaces/83147/project/1611251"><i class="fas fa-cloud"></i> Class Size Regression Analysis</a></li>
</ul>
<h2 id="slides">Slides</h2>
<ul>
<li><a href="/slides/2.4-slides.html"><i class="fas fa-chalkboard-teacher"></i> View Lecture Slides</a></li>
<li><a href="/slides/2.4-slides.pdf"><i class="fas fa-file-pdf"></i> Download as PDF</a></li>
</ul>
<h2 id="live-class-session-on-zoom">Live Class Session on Zoom</h2>
<p>The live class <i class="fas fa-video"></i> Zoom meeting link can be found on Blackboard (see <code>LIVE ZOOM MEETINGS</code> on the left navigation menu), starting at 11:30 AM.</p>
<p>If you are unable to join today’s live session, or if you want to review, you can find the recording stored on Blackboard via Panopto (see <code>Class Recordings</code> on the left navigation menu).</p>
<h2 id="problem-set">Problem Set</h2>
<p><a href="/assignment/01-problem-set">Problem Set 1</a> answers are posted on that page in various formats.</p>
<p><a href="/assignment/02-problem-set">Problem set 2</a> answers are posted on that page in various formats.</p>
<h2 id="appendix-ols-estimators">Appendix: OLS Estimators</h2>
<h3 id="deriving-the-ols-estimators">Deriving the OLS Estimators</h3>
<p>The population linear regression model is:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i + u _i\]</span></p>
<p>The errors <span class="math inline">\((u_i)\)</span> are unobserved, but for candidate values of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, we can obtain an estimate of the residual. Algebraically, the error is:</p>
<p><span class="math display">\[\hat{u_i}=    Y_i-\hat{\beta_0}-\hat{\beta_1}X_i\]</span></p>
<p>Recall our goal is to find <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> that <em>minimizes</em> the sum of squared errors (SSE):</p>
<p><span class="math display">\[SSE= \sum^n_{i=1} \hat{u_i}^2\]</span></p>
<p>So our minimization problem is:</p>
<p><span class="math display">\[\min_{\hat{\beta_0}, \hat{\beta_1}} \sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1}X_i)^2\]</span></p>
<p>Using calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial SSE}{\partial \hat{\beta_0}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\\
\frac{\partial SSE}{\partial \hat{\beta_1}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\\
\end{align*}\]</span></p>
<h4 id="finding-hatbeta_0">Finding <span class="math inline">\(\hat{\beta_0}\)</span></h4>
<p>Working with the first FOC, divide both sides by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\]</span></p>
<p>Then expand the summation across all terms and divide by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\underbrace{\frac{1}{n}\sum^n_{i=1} Y_i}_{\bar{Y}}-\underbrace{\frac{1}{n}\sum^n_{i=1}\hat{\beta_0}}_{\hat{\beta_0}}-\underbrace{\frac{1}{n}\sum^n_{i=1} \hat{\beta_1} X_i}_{\hat{\beta_1}\bar{X}}=0\]</span></p>
<p>Note the first term is <span class="math inline">\(\bar{Y}\)</span>, the second is <span class="math inline">\(\hat{\beta_0}\)</span>, the third is <span class="math inline">\(\hat{\beta_1}\bar{X}\)</span>.<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">From the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operator">rules about summation operators</a>, we define the mean of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(\bar{X}=\frac{1}{n}\displaystyle\sum_{i=1}^n X_i\)</span>. The mean of a constant, like <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> is itself.<br />
<br />
</span></span></p>
<p>So we can rewrite as:
<span class="math display">\[\bar{Y}-\hat{\beta_0}-\beta_1=0\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[\hat{\beta_0}=\bar{Y}-\bar{X}\beta_1\]</span></p>
<h4 id="finding-hatbeta_1">Finding <span class="math inline">\(\hat{\beta_1}\)</span></h4>
<p>To find <span class="math inline">\(\hat{\beta_1}\)</span>, take the second FOC and divide by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\]</span></p>
<p>From the formula for <span class="math inline">\(\hat{\beta_0}\)</span>, substitute in for <span class="math inline">\(\hat{\beta_0}\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg(Y_i-[\bar{Y}-\hat{\beta_1}\bar{X}]-\hat{\beta_1} X_i\bigg)X_i=0\]</span></p>
<p>Combining similar terms:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg([Y_i-\bar{Y}]-[X_i-\bar{X}]\hat{\beta_1}\bigg)X_i=0\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> and expand terms into the subtraction of two sums (and pull out <span class="math inline">\(\hat{\beta_1}\)</span> as a constant in the second sum:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i-\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i=0\]</span></p>
<p>Move the second term to the righthand side:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i\]</span></p>
<p>Divide to keep just <span class="math inline">\(\hat{\beta_1}\)</span> on the right:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i}{\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i}=\hat{\beta_1}\]</span></p>
<p>Note that from the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operatorproperties">rules about summation operators</a>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\]</span></p>
<p>and:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [X_i-\bar{X}]X_i=\displaystyle\sum^n_{i=1} (X_i-\bar{X})(X_i-\bar{X})=\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2\]</span></p>
<p>Plug in these two facts:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2}=\hat{\beta_1}\]</span></p>
<h3 id="algebraic-properties-of-ols-estimators">Algebraic Properties of OLS Estimators</h3>
<p>The OLS residuals <span class="math inline">\(\hat{u}\)</span> and predicted values <span class="math inline">\(\hat{Y}\)</span> are chosen by the minimization problem to satisfy:</p>
<ol style="list-style-type: decimal">
<li><p>The expected value (average) error is 0:
<span class="math display">\[E(u_i)=\frac{1}{n}\displaystyle \sum_{i=1}^n \hat{u_i}=0\]</span></p></li>
<li><p>The covariance between <span class="math inline">\(X\)</span> and the errors is 0:
<span class="math display">\[\hat{\sigma}_{X,u}=0\]</span></p></li>
</ol>
<p>Note the first two properties imply strict <strong>exogeneity</strong>. That is, this is only a valid model if <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> are not correlated.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>The expected predicted value of <span class="math inline">\(Y\)</span> is equal to the expected value of <span class="math inline">\(Y\)</span>:
<span class="math display">\[\bar{\hat{Y}}=\frac{1}{n} \displaystyle\sum_{i=1}^n \hat{Y_i} = \bar{Y}\]</span></p></li>
<li><p>Total sum of squares is equal to the explained sum of squares plus sum of squared errors:
<span class="math display">\[\begin{align*}TSS&amp;=ESS+SSE\\
\sum_{i=1}^n (Y_i-\bar{Y})^2&amp;=\sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n {u}^2\\
\end{align*}\]</span></p></li>
</ol>
<p>Recall <span class="math inline">\(R^2\)</span> is <span class="math inline">\(\frac{ESS}{TSS}\)</span> or <span class="math inline">\(1-SSE\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li>The regression line passes through the point <span class="math inline">\((\bar{X},\bar{Y})\)</span>, i.e. the mean of <span class="math inline">\(X\)</span> and the mean of <span class="math inline">\(Y\)</span>.</li>
</ol>
<h3 id="bias-in-hatbeta_1">Bias in <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with the formula we derived for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Recall from <strong>Rule 6</strong> of summations, we can rewrite the numerator as</p>
<p><span class="math display">\[\begin{align*}
    =&amp;\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\\
    =&amp; \displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})\\
\end{align*}\]</span></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We know the true population relationship is expressed as:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i+u_i\]</span></p>
<p>Substituting this in for <span class="math inline">\(Y_i\)</span> in equation 2:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (\beta_0+\beta_1X_i+u_i)(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span>
Breaking apart the sums in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})+\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})+\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We can simplify equation 4 using <strong>Rules 4 and 5</strong> of summations</p>
<ol style="list-style-type: decimal">
<li>The first term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_0\)</span>, which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per <strong>Rule 4</strong>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})&amp;= \beta_0 \displaystyle \sum^n_{i=1} (X_i-\bar{X})\\
&amp;=\beta_0 (0)\\
&amp;=0\\
\end{align*}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The second term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_1\)</span>, which can be pulled out of the summation. Additionally, <strong>Rule 5</strong> tells us <span class="math inline">\(\displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})=\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_1X_1(X_i-\bar{X})&amp;= \beta_1 \displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})\\
&amp;=\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\\
\end{align*}\]</span></p>
<p>When placed back in the context of being the numerator of a fraction, we can see this term simplifies to just <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \frac{\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} &amp;=\frac{\beta_1}{1} \times \frac{\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\\
    &amp;=\beta_1   \\
\end{align*}\]</span></p>
<p>Thus, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Now, take the expectation of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E\left[\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>We can break this up, using properties of expectations. First, recall <span class="math inline">\(E[a+b]=E[a]+E[b]\)</span>, so we can break apart the two terms.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1]+E\left[\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>Second, the true population value of <span class="math inline">\(\beta_1\)</span> is a constant, so <span class="math inline">\(E[\beta_1]=\beta_1\)</span>.</p>
<p>Third, since we assume <span class="math inline">\(X\)</span> is also “fixed” and not random, the variance of <span class="math inline">\(X\)</span>, <span class="math inline">\(\displaystyle\sum_{i=1}^n (X_i-\bar{X})\)</span>, in the denominator, is just a constant, and can be brought outside the expectation.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\frac{E\left[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\right]  }{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Thus, the properties of the equation are primarily driven by the expectation <span class="math inline">\(E\bigg[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\bigg]\)</span>. We now turn to this term.</p>
<p>Use the property of summation operators to expand the numerator term:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
        \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
\end{align*}\]</span></p>
<p>Now divide the numerator and denominator of the second term by <span class="math inline">\(\frac{1}{n}\)</span>. Realize this gives us the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> in the numerator and variance of <span class="math inline">\(X\)</span> in the denominator, based on their respective definitions.</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\cfrac{\frac{1}{n}\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\frac{1}{n}\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{cov(X,u)}{var(X)} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{s_{X,u}}{s_X^2} \\
\end{align*}\]</span></p>
<p>By the Zero Conditional Mean assumption of OLS, <span class="math inline">\(s_{X,u}=0\)</span>.</p>
<p>Alternatively, we can express the bias in terms of correlation instead of covariance:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\cfrac{cov(X,u)}{var(X)}\]</span></p>
<p>From the definition of correlation:</p>
<p><span class="math display">\[\begin{align*}
     cor(X,u)&amp;=\frac{cov(X,u)}{s_X s_u}\\
     cor(X,u)s_Xs_u &amp;=cov(X,u)\\
\end{align*}\]</span></p>
<p>Plugging this in:</p>
<p><span class="math display">\[\begin{align*}
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cov(X,u)}{var(X)} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{\big[cor(X,u)s_xs_u\big]}{s^2_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cor(X,u)s_u}{s_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+cor(X,u)\frac{s_u}{s_X} \\
\end{align*}\]</span></p>
<h3 id="proof-of-the-unbiasedness-of-hatbeta_1">Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with equation:<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Admittedly, this is a simplified version where <span class="math inline">\(\hat{\beta_0}=0\)</span>, but there is no loss of generality in the results.<br />
<br />
</span></span></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum Y_iX_i}{\sum X_i^2}\]</span></p>
<p>Substitute for <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum (\beta_1 X_i+u_i)X_i}{\sum X_i^2}\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2+u_iX_i}{\sum X_i^2}\]</span></p>
<p>Separate the sum into additive pieces:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is constant, so we can pull it out of the first sum:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 \frac{\sum X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Simplifying the first term, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 +\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Now if we take expectations of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1] +E\left[\frac{u_i X_i}{\sum X_i^2}\right]\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is a constant, so the expectation of <span class="math inline">\(\beta_1\)</span> is itself.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +E\bigg[\frac{u_i X_i}{\sum X_i^2}\bigg]\]</span></p>
<p>Using the properties of expectations, we can pull out <span class="math inline">\(\frac{1}{\sum X_i^2}\)</span> as a constant:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2} E\bigg[\sum u_i X_i\bigg]\]</span></p>
<p>Again using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2}\sum E[u_i X_i]\]</span></p>
<p>Under the exogeneity condition, the correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span> is 0.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1\]</span></p>
