---
title: "2.6 — Statistical Inference — Class Notes"
citeproc: false
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-syllabus-no-bib.csl
output:
  blogdown::html_page:
    template: ../../pandoc/toc-title_html.template
    toc: true
editor_options: 
  chunk_output_type: console
---

<!-- BLOGDOWN-HEAD -->
<script src="/rmarkdown-libs/accessible-code-block/empty-anchor.js"></script>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>
<!-- /BLOGDOWN-HEAD -->

<h2>Contents</h2>
<div id="TOC">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#live-class-session-on-zoom">Live Class Session on Zoom</a></li>
<li><a href="#practice-problems">Practice Problems</a></li>
<li><a href="#assignments-problem-set-3-midterm-exam">Assignments: Problem Set 3 &amp; Midterm Exam</a></li>
<li><a href="#new-packages-mentioned">New Packages Mentioned</a></li>
<li><a href="#assignments-problem-set-3-due-sunday-september-27-midterm">Assignments: Problem Set 3 Due Sunday September 27, Midterm</a></li>
<li><a href="#appendix-inferential-statistics">Appendix: Inferential Statistics</a></li>
<li><a href="#inferential-statistics">Inferential Statistics</a><ul>
<li><a href="#differences-between-what-we-learned-in-class-and-classical-statistics">Differences Between What We Learned in Class and Classical Statistics</a></li>
<li><a href="#inferential-statistics-basics">Inferential Statistics Basics</a></li>
<li><a href="#confidence-intervals">Confidence Intervals</a></li>
</ul></li>
</ul>
</div>

<p><em>Tuesday, September 22, 2020</em></p>
<h2 id="overview">Overview</h2>
<p>We begin with some more time for you to work on the <a href="/r/2.5-r-practice">R Practice</a> from last class.</p>
<p>This class will be spread over 2 days, and is about inferential statistics: using <em>statistics</em> calculated from a <em>sample</em> of data to <strong>infer</strong> the true (and unmeasurable) <em>parameters</em> that describe a <em>population.</em> In doing so, we can run <em>hypothesis tests</em> on our sample to determine a <em>point</em> estimate of a parameter, or construct a <em>confidence interval</em> from our sample to cast a range for the true parameter.</p>
<p>This is standard principles of statistics - you hopefully should have learned it before. If it has been a while (or never) since your last statistics class, this is one of the hardest concepts to understand at first glance. I recommend <a href="https://www.khanacademy.org/math/statistics-probability">Khan Academy</a><span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">From sampling distributions through significance tests, for this. Though the whole class is helpful!<br />
<br />
</span></span> or Google for these concepts, as <em>every</em> statistics class will cover them in the standard way.</p>
<p>That being said, this semester I am <em>not</em> going to cover them in the standard way (see the appendix today below for an overview of the standard way). I think it will be more intuitive if I <em>show</em> you where these concepts come from by <em>simulation</em> as opposed to theoretical sampling distributions.</p>
<h2 id="slides">Slides</h2>
<ul>
<li><a href="/slides/2.6-slides.html"><i class="fas fa-chalkboard-teacher"></i> View Lecture Slides</a></li>
<li><a href="/slides/2.6-slides.pdf"><i class="fas fa-file-pdf"></i> Download as PDF</a></li>
</ul>
<h2 id="live-class-session-on-zoom">Live Class Session on Zoom</h2>
<p>The live class <i class="fas fa-video"></i> Zoom meeting link can be found on Blackboard (see <code>LIVE ZOOM MEETINGS</code> on the left navigation menu), starting at 11:30 AM.</p>
<p>If you are unable to join today’s live session, or if you want to review, you can find the recording stored on Blackboard via Panopto (see <code>Class Recordings</code> on the left navigation menu).</p>
<h2 id="practice-problems">Practice Problems</h2>
<p>You will be finishing up <a href="/r/2.5-r-practice.html">R practice problems</a> from last class. Answers are posted on that page.</p>
<h2 id="assignments-problem-set-3-midterm-exam">Assignments: Problem Set 3 &amp; Midterm Exam</h2>
<p><a href="/assignment/03-problem-set.html">Problem Set 3</a> is posted and is due by 11:59 PM Sunday September 27 by upload on Blackboard. Please see that page for more information on how to submit (there are multiple ways!).</p>
<p>The midterm exam (on 1.1-2.7) will be next week. I will provide more information this week. We will dedicate time for review and questions on class Tuesday, September 29</p>
<h2 id="new-packages-mentioned">New Packages Mentioned</h2>
<ul>
<li><code>infer</code>: for simulation for statistical inference</li>
</ul>
<h2 id="assignments-problem-set-3-due-sunday-september-27-midterm">Assignments: Problem Set 3 Due Sunday September 27, Midterm</h2>
<p><a href="/assignment/03-problem-set">Problem Set 3</a> will be due Thursday October 10.</p>
<p>Exam 1 (on 1.1-2.7) will be next week (week of September 28), I will provide more details next class. We save some time for review on Tuesday, September 29</p>
<h2 id="appendix-inferential-statistics">Appendix: Inferential Statistics</h2>
<h2 id="inferential-statistics">Inferential Statistics</h2>
<h3 id="differences-between-what-we-learned-in-class-and-classical-statistics">Differences Between What We Learned in Class and Classical Statistics</h3>
<p>In class, you learned the basics behind inferential statistics–p-values, confidence intervals, hypothesis testing–via <em>empirical simulation</em> of many samples permuted from our existing data. We took our sample, ran 1,000 simulations by permutation of our sample without replacement<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">That is, for each simulation, we randomly selected observations from our existing sample to be in the simulation, and then did <em>not</em> put that observation back in the pool to possibly be selected again.<br />
<br />
</span></span>, calculated the statistic (<span class="math inline">\(\hat{\beta}_0\)</span>, the slope) of each simulation; this gave us a (sampling) distribution of our sample statistics, and then found the probability on that distribution that we would observe our <em>actual</em> statistic in our actual data – this is the <span class="math inline">\(p\)</span>-value.</p>
<p>Classically, before the use of computers that could run and visualize 1,000s of simulations within seconds,<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Even when <em>I</em> was in graduate school, 2011–2015<br />
<br />
</span></span> inferential statistics was taught using <em>theoretical</em> distributions. <strong>Essentially, we calculate a test-statistic by normalizing our finding against a theoretical (null) sampling distribution of our sample statistic, and find p-values by estimating the probability of observing that statistic on that theoretical distribution.</strong> These distributions are almost always <em>normal</em> or <em>normal-like</em> distributions. The distribution that we almost always use in econometrics is the (Student’s) <span class="math inline">\(t\)</span>-distribution.</p>
<p>Furthermore, testing the null hypothesis <span class="math inline">\(H_0: \,\beta_1=0\)</span>, is not the only type of hypothesis test, nor is the slope the only statistic we can test. In fact, there are many different types of hypothesis tests that are well-known and well-used, we focused entirely on regression (since that is the largest tool of the course).</p>
<p>This appendix will give you more background on the theory of inferential statistics, and is more in line with what you may have learned in earlier statistics courses.</p>
<h3 id="inferential-statistics-basics">Inferential Statistics Basics</h3>
<h4 id="prerequisites">Prerequisites</h4>
<p>It is important to remember several statistical distributions, tools, and facts. Most of them have to do with the <strong>normal distribution</strong>. If <span class="math inline">\(X\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[X \sim N(\mu, \sigma)\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## ── Attaching packages ────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.1     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0</code></pre>
<pre><code>## ── Conflicts ───────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># plot a theoretical normal distribution without any data</span></span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># our &quot;geom&quot; here is the &quot;stat_function()&quot; command, which draws statistical functions</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>normal.pdf &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">-4</span><span class="op">:</span><span class="dv">4</span>), <span class="co"># our &quot;data&quot; is just a sequence of x from -4 to 4</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>                     <span class="kw">aes</span>(<span class="dt">x =</span> x))<span class="op">+</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="co"># dnorm is the normal distributions</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>                <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="co"># set mean = 0 and sd = 1</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>                <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>)<span class="op">+</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>))<span class="op">+</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;X (or Z-score)&quot;</span>)<span class="op">+</span><span class="kw">ylab</span>(<span class="st">&quot;p(X)&quot;</span>)<span class="op">+</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="st">  </span><span class="kw">theme_light</span>()</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a>normal.pdf</span></code></pre></div>
<p><img src="/class/2.6-class_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Then recall the <strong>68-95-99.7 empirical rule</strong>, that:</p>
<ul>
<li><span class="math inline">\(P(\mu-1 \sigma \leq X \leq \mu+ 1\sigma) \approx 0.68\)</span></li>
<li><span class="math inline">\(P(\mu-2 \sigma \leq X \leq \mu+ 2\sigma) \approx 0.95\)</span></li>
<li><span class="math inline">\(P(\mu-3 \sigma \leq X \leq \mu+ 3\sigma) \approx 0.997\)</span></li>
</ul>
<p>Again, in English: “68% of the observations fall within 1 standard deviation of the mean; 95% fall within 2 standard deviations of the mean, and 99.7% fall within 3 standard deviations of the mean.”</p>
<p>If we have the <strong>standard normal distribution</strong> with mean 0 and standard deviation 1:</p>
<p><img src="/class/2.6-class_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Again, we can <em>standardize</em> any normally-distributed random variable by finding the <strong>Z-score</strong> of each observation:</p>
<p><span class="math display">\[Z=\frac{X_i - \mu}{\sigma}\]</span></p>
<p>This ensures the mean will be 0 and standard deviation will be 1. Thus, <span class="math inline">\(Z\)</span> is the number of standard deviations above <span class="math inline">\((+)\)</span> or below <span class="math inline">\((-)\)</span> the mean an observation is.</p>
<p>We can use <span class="math inline">\(Z\)</span>-scores to find the probability of any range of observations of <span class="math inline">\(X\)</span> occuring in the distribution.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>) <span class="co"># area to left of -2</span></span></code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">pnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>) <span class="co"># area to left of 2</span></span></code></pre></div>
<pre><code>## [1] 0.9772499</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">pnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>) <span class="co"># area to RIGHT of 2</span></span></code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">pnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>)<span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>) <span class="co"># area between -2 and 2</span></span></code></pre></div>
<pre><code>## [1] 0.9544997</code></pre>
<h4 id="the-central-limit-theorem">The Central Limit Theorem</h4>
<p>Inferential statistics can be summarized in 2 sentences:</p>
<blockquote>
<p>There are unknown <strong>parameters</strong> that describe a <strong>population</strong> distribution that we want to know. We use <strong>statistics</strong> that describe a <strong>sample</strong> to <em>estimate</em> the population parameters.</p>
</blockquote>
<p>Recall there is an element of randomness in our sample statistics due to <strong>sampling variability</strong>. For example, if we take the mean of one sample, <span class="math inline">\(\bar{x}\)</span>, and then take the mean of a <em>different</em> sample, the <span class="math inline">\(\bar{x}\)</span>’s will be <em>slightly different</em>. We can concieve of a distribution of <span class="math inline">\(\bar{x}\)</span>’s across many different samples, and this is called the <strong>sampling distribution</strong> of the statistic <span class="math inline">\((\bar{x})\)</span>.</p>
<p>Via the sampling distribution, the sample statistic <span class="math inline">\((\bar{X})\)</span> itself is distributed with</p>
<ul>
<li>mean <span class="math inline">\(E[\bar{x}]=\mu_X\)</span> (the true population mean)</li>
<li>standard deviation <span class="math inline">\(\sigma_{\bar{x}}\)</span></li>
</ul>
<p><strong>Central Limit Theorem</strong>: <em>with large enough sample size (<span class="math inline">\(n\geq30\)</span>), the sampling distribution of a sample statistic is approximately normal</em><span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">If samples are i.i.d. (independently and identically distributed if they are drawn from the same population randomly and then replaced) we don’t even need to know the population distribution to assume normality<br />
<br />
</span></span></p>
<p>Thus, the <strong>sampling distribution of the sample mean</strong> (<span class="math inline">\(\bar{x}\)</span>):</p>
<p><span class="math display">\[\bar{X} \sim \left(\mu_X, \frac{\sigma_X}{\sqrt{n}}\right)\]</span></p>
<p>The second term we call the <strong>standard error of the sample mean</strong><span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">Instead of the “standard deviation”. “Standard error” refers to the sampling variability of a sample statistic, and is always talking about a sampling distribution.<br />
<br />
</span></span>. Note that it takes the true standard deviation of the population (<span class="math inline">\(\sigma_X\)</span>)<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">Which we need to know! We often do not know it!<br />
<br />
</span></span> and divides it by the square root of the sample size, <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>Thus <strong>if we know the true population standard deviation</strong> (<span class="math inline">\(\sigma_X\)</span>) then we can simply use the normal distribution for confidence intervals and hypothesis tests of a sample statistic. Since we often do not, we need to use another distribution for inferential statistics, often the <span class="math inline">\(t\)</span>-distribution.</p>
<h4 id="if-we-dont-know-sigma-the-students-t-distribution">If We Don’t Know <span class="math inline">\(\sigma\)</span>: The Student’s <span class="math inline">\(t\)</span>-Distribution</h4>
<p>We rarely, if ever, know the true population standard deviation for variable <span class="math inline">\(X\)</span>, <span class="math inline">\(\sigma_X\)</span>. Additionally, we sometimes have sample sizes of <span class="math inline">\(n &lt; 30\)</span>. If either of these conditions are true, we cannot use leverage the Central Limit Theorem and simplify with a standard normal distribution. Instead of the normal distribution, we use a <strong>Student’s t-Distribution</strong><span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">“Student” was the penname of William Sealy Gosset, who has one of the more interesting stories in statistics. He worked for Guiness in Ireland testing the quality of beer. He found that with small sample sizes, normal distributions did not yield accurate results. He came up with a more accurate distribution, and since Guiness would not let him publish his findings, published it under the pseudonym of “Student.”<br />
<br />
</span></span></p>
<p><span class="math inline">\(t\)</span> is functionally equivalent to the idea of a <span class="math inline">\(Z\)</span>-score, with some slight modifications:</p>
<p><span class="math display">\[t = \cfrac{\bar{x}-\mu}{\left(\frac{s}{\sqrt{n}}\right)}\]</span>
- <span class="math inline">\(\bar{x}\)</span> is our estimated statistic (e.g. sample mean)
- <span class="math inline">\(\mu\)</span> is the true population parametner (e.g. population mean)
- <span class="math inline">\(s\)</span> is the sample standard deviation
- <span class="math inline">\(n\)</span> is the sample size</p>
<p><span class="math inline">\(t\)</span>-scores similarly measure the number of standard deviations an observation is above or below the mean.</p>
<p>The other main difference between normal distributions/<span class="math inline">\(Z\)</span>-scores and <span class="math inline">\(t\)</span> distributions /<span class="math inline">\(t\)</span>-scores is that <span class="math inline">\(t\)</span> distributions have <span class="math inline">\(n-1\)</span> degrees of freedom.<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote"><strong>Degrees of freedom</strong>, <span class="math inline">\(df\)</span> are the number of independent values used for the calculation of a statistic minus the number of other statistics used as intermediate steps. For sample standard deviation <span class="math inline">\(s\)</span>, we use <span class="math inline">\(n\)</span> deviations <span class="math inline">\((x_i-\bar{x})\)</span> and 1 parameter <span class="math inline">\((\bar{x})\)</span>, hence <span class="math inline">\(df=n-1\)</span><br />
<br />
</span></span></p>
<p><span class="math display">\[t \sim t_{n-1}\]</span></p>
<pre><code>## Loading required package: viridisLite</code></pre>
<p><img src="/class/2.6-class_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The standard <span class="math inline">\(t\)</span>-distribution looks normal-ish, with a mean of 0, but with more area in the tails of the distribution. The exact shape depends on the degrees of freedom <span class="math inline">\((n-1)\)</span>. As <span class="math inline">\(\uparrow n\)</span>, <span class="math inline">\(\uparrow df\)</span>, the <span class="math inline">\(t\)</span>-distribution approximates a normal distribution.</p>
<p>By convention, in regression we <em>always</em> use <span class="math inline">\(t\)</span>-distributions for confidence intervals and hypothesis tests. For nearly all of the confidence intervals and hypothesis tests below, we functionally replace <span class="math inline">\(Z\)</span> with <span class="math inline">\(t\)</span>.</p>
<h3 id="confidence-intervals">Confidence Intervals</h3>
<p>A <strong>confidence interval</strong> describes the range of estimates for a population parameter in the form:</p>
<p><span class="math display">\[(\text{estimate} - \text{margin of error}, \, \text{estimate} + \text{margin of error})\]</span></p>
<p>Our <em>confidence level</em> is <span class="math inline">\(1-\alpha\)</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span> again is the “significance level”, the probability that the true population parameter is <em>not</em> within our confidence interval<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote">Equivalently, <span class="math inline">\(\alpha\)</span> is the probability of a <strong>Type I error</strong>: a false positive finding where we incorrectly reject a null hypothesis when it the null hypothesis is in fact true.<br />
<br />
</span></span></li>
<li>Typical confidence levels: 90%, 95%, 99%</li>
</ul>
<blockquote>
<p>A confidence interval tells us that if we were to conduct many samples, (<span class="math inline">\(1-\alpha\)</span>)% would contain the true population parameter within the interval</p>
</blockquote>
<p>To construct a confidence interval, we do the following:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Calculate the sample statistic.</strong></p></li>
<li><p><strong>Find <span class="math inline">\(Z\)</span>-score that corresponds to desired confidence level.</strong><span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">Of course, if we don’t know the population <span class="math inline">\(\sigma\)</span>, we need to use the <span class="math inline">\(t\)</span>-distribution and find critical <span class="math inline">\(t\)</span>-scores instead of <span class="math inline">\(Z\)</span>-scores. See above.<br />
<br />
</span></span> We need to find what are called the <strong>“critical values”</strong> of <span class="math inline">\(Z\)</span>, which we will call <span class="math inline">\(Z_{0.5\alpha}\)</span> on the normal distribution that puts (<span class="math inline">\(1-\alpha\)</span>) probability between <span class="math inline">\(\pm Z_{0.5\alpha}\)</span> and <span class="math inline">\(0.5\alpha\)</span> in each of the tails of the distribution. The distribution would look like this:</p></li>
</ol>
<pre><code>## Warning in is.na(x): is.na() applied to non-(list or vector) of type
## &#39;expression&#39;

## Warning in is.na(x): is.na() applied to non-(list or vector) of type
## &#39;expression&#39;

## Warning in is.na(x): is.na() applied to non-(list or vector) of type
## &#39;expression&#39;</code></pre>
<p><img src="/class/2.6-class_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<ul>
<li>The confidence interval between the two <span class="math inline">\(Z\)</span>-scores <span class="math inline">\(-Z_{0.5\alpha}\)</span> and <span class="math inline">\(Z_{0.5\alpha}\)</span> contains the desired <span class="math inline">\((1-\alpha)%\)</span> of observations</li>
<li>The area beyond each <span class="math inline">\(Z\)</span>-score contains <span class="math inline">\(0.5\alpha\)</span>% of observations in each direction, for a total of <span class="math inline">\(\alpha\)</span>% beyond the critical values of <span class="math inline">\(Z\)</span></li>
</ul>
<p>Note that the image above is abstract. So for example, if we wanted a (typical) <strong>95% confidence interval</strong> with <span class="math inline">\(\alpha=0.05\)</span>, the critical value(s) of <span class="math inline">\(Z\)</span> are <span class="math inline">\(\pm 1.96\)</span><span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle"/><span class="sidenote">Note this is the precise value behind the rule of thumb that 95% of observations fall within 2 standard deviations of the mean!<br />
<br />
</span></span>, and looking on the distribution:</p>
<p><img src="/class/2.6-class_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The critical values of <span class="math inline">\(Z\)</span> are often given in <span class="math inline">\(Z\)</span>-tables, which you can find in classical statistics textbooks or online. Critical values of <span class="math inline">\(Z\)</span> for common confidence intervals values are well known:</p>
<table>
<thead>
<tr class="header">
<th>Confidence Level</th>
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(\pm Z_{0.5\alpha}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>90%</td>
<td>0.10</td>
<td><span class="math inline">\(\pm 1.645\)</span></td>
</tr>
<tr class="even">
<td>95%</td>
<td>0.05</td>
<td><span class="math inline">\(\pm 1.96\)</span></td>
</tr>
<tr class="odd">
<td>99%</td>
<td>0.01</td>
<td><span class="math inline">\(\pm 2.58\)</span></td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: decimal">
<li><strong>Calculate the margin of error (MOE)</strong></li>
</ol>
<p>The margin of error is the critical value of <span class="math inline">\(Z\)</span> times the standard error of the estimate (<span class="math inline">\(\sigma\)</span>).</p>
<p><span class="math display">\[MOE=Z_{0.5\alpha}\sigma\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Construct the confidence interval</strong></li>
</ol>
<p>The confidence interval is simply our estimate plus and minus the margin of error.</p>
<p><span class="math display">\[CI = \left([\bar{x}-Z_{0.5}\sigma_{\bar{x}}], \, [\bar{x}+Z_{0.5}\sigma_{\bar{x}}]\right)\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Intepret the confidence interval in the context of the problem</strong></li>
</ol>
<blockquote>
<p>“We estimate with [1-alpha]% confidence that the true [population parameter] is between [lowerbound] and [upperbound]”.</p>
</blockquote>
